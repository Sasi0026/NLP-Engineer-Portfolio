# Telugu Text Summarization using MBART

This repository presents a complete pipeline for developing a Telugu text summarization model based on MBART and LoRA/PEFT techniques. It covers data collection, preprocessing, model training, evaluation, and deployment with a Gradio UI.

## Research & Motivation

- **Literature Survey:** Researched the field of Telugu text summarization and reviewed 20+ relevant research papers.
- **Dataset Exploration:** Studied and referenced the TeSum dataset, specifically designed for Telugu summarization tasks.
- **Data Augmentation:** Collected 1000+ additional Telugu news articles and annotated them with summaries generated by a pretrained model, ensuring diversity by shuffling and mixing them with existing data.

## Data Collection

- Used **Scrapy** for web scraping Telugu news articles.
- Defined simple spiders for efficient data collection.
- Relied on Scrapy's built-in data cleaning; no extensive preprocessing was required.

## Preprocessing

- **Tokenization:** Utilized HuggingFace's `AutoTokenizer` for converting text to word embeddings compatible with MBART.
- Minimal additional cleaning due to Scrapy's preprocessing.

## Model Training

- **Model:** Loaded and initialized MBART from HuggingFace's Transformers.
- **PEFT & LoRA:** Applied Parameter Efficient Fine Tuning (PEFT) and LoRA, freezing most model parameters and updating only 1.5% for summarization.
- **Training Details:**
  - Initial training: 1500 examples, 8 epochs, ROUGE-1 score: **0.21**
  - Extended training: 7000 examples, 6 epochs, ROUGE-1 score: **0.27**, ROUGE-L score: **0.29**
  - Average summary length: **30 words** per sentence

## Evaluation

- Performance measured using ROUGE scores (ROUGE-1, ROUGE-L).
- Validation conducted at each training stage.

## Deployment

- Model saved and reloaded for inference.
- Deployed a simple Gradio UI for real-time Telugu text summarization.

## Future Work & Enhancements

1. **RLHF Fine-Tuning:** Plan to implement Reinforcement Learning from Human Feedback (RLHF) for further fine-tuning and aligning model outputs with human preferences.
2. **DPO Alignment:** Currently working on Direct Preference Optimization (DPO) to improve model alignment.



**Contact:** [![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue)]
- [LinkedIn](www.linkedin.com/in/sasi-kiran18)
